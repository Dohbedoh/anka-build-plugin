current situation
---------------------
regular pipeline and freestyle jobs work as expected
weird behaviour to consider: if you run a basic job, cancel it before jenkins node has not started yet (but after start vm has been sent to controller)
    What happens is that jenkins cancels the job but the node and vm get generated and connected to one another.
    They die when jenkins kills the idle node, or they get picked up if same job is started again.
    We need to think if this is a normal behaviour for us or not.

current issues
----------------
- daemon instance behaviour on jenkins restarts and config changes:
    config changes at the moment do not create new instance daemons (good)
    restarts lose all data saved in the daemons and zombie vms are left
- handle errors better on getStatus() called from instance daemon

to be tested
--------------
- cache builder jobs
- dynamic slave jobs
- restart resilience
- check regression for running two parallel jobs in pipeline that one fails while the other runs

more to do
-------------
- add tests for zombie vms